{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "587e28ef",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Verify environment and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badfc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check Python version\n",
    "print(f\"Python version: {sys.version}\")\n",
    "assert sys.version_info >= (3, 10), \"Python 3.10+ required\"\n",
    "\n",
    "# Import core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Video\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Print memory stats\n",
    "    print(f\"\\nGPU Memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"GPU Memory cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  WARNING: No GPU detected. Heavy inference steps will be skipped.\")\n",
    "    print(\"   For full functionality, run on a machine with NVIDIA GPU + CUDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c45f4d6",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set all parameters in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af155c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION CELL - Edit parameters here\n",
    "# ============================================\n",
    "\n",
    "# Case/experiment name\n",
    "CASE_NAME = \"dog_test_01\"\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path(\"../\").resolve()\n",
    "INPUT_DIR = PROJECT_ROOT / \"data\" / \"raw\" / CASE_NAME\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / CASE_NAME\n",
    "\n",
    "# Image processing\n",
    "MAX_IMAGES = 50  # Maximum images to process\n",
    "IMAGE_SIZE = 512  # Target image size for processing\n",
    "MIN_IMAGE_SIZE = 256  # Minimum acceptable image size\n",
    "\n",
    "# Quality thresholds\n",
    "BLUR_THRESHOLD = 100.0  # Laplacian variance threshold (lower = more blurry)\n",
    "PERCEPTUAL_HASH_THRESHOLD = 5  # Hamming distance for duplicate detection\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model configuration\n",
    "HUNYUAN_MODEL_ID = \"tencent/Hunyuan3D-2\"\n",
    "ENABLE_TEXTURE = True  # Set to False to skip texture generation (saves VRAM)\n",
    "\n",
    "# Output settings\n",
    "TURNTABLE_FRAMES = 36  # Number of frames for turntable video\n",
    "TURNTABLE_FPS = 12  # Frames per second for turntable video\n",
    "\n",
    "# Display configuration\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Case name: {CASE_NAME}\")\n",
    "print(f\"  Input dir: {INPUT_DIR}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}px\")\n",
    "print(f\"  Texture generation: {'Enabled' if ENABLE_TEXTURE else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f4a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / \"images\").mkdir(exist_ok=True)\n",
    "(OUTPUT_DIR / \"meshes\").mkdir(exist_ok=True)\n",
    "(OUTPUT_DIR / \"renders\").mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Output directories created at: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fde65b",
   "metadata": {},
   "source": [
    "## 3. Load Images\n",
    "\n",
    "Load all images from the input directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee214f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input directory exists\n",
    "if not INPUT_DIR.exists():\n",
    "    print(f\"âš ï¸  Input directory does not exist: {INPUT_DIR}\")\n",
    "    print(f\"\\nPlease create the directory and add images:\")\n",
    "    print(f\"  mkdir -p {INPUT_DIR}\")\n",
    "    print(f\"  # Then copy your .jpg/.png images to {INPUT_DIR}\")\n",
    "    raise FileNotFoundError(f\"Input directory not found: {INPUT_DIR}\")\n",
    "\n",
    "# Find all image files\n",
    "image_extensions = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']\n",
    "image_files = []\n",
    "\n",
    "for ext in image_extensions:\n",
    "    image_files.extend(list(INPUT_DIR.glob(f\"*{ext}\")))\n",
    "\n",
    "image_files = sorted(image_files)[:MAX_IMAGES]\n",
    "\n",
    "print(f\"Found {len(image_files)} images in {INPUT_DIR}\")\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    print(f\"\\nâš ï¸  No images found in {INPUT_DIR}\")\n",
    "    print(f\"   Supported formats: {', '.join(image_extensions)}\")\n",
    "    raise ValueError(\"No images found\")\n",
    "\n",
    "# Display first few filenames\n",
    "print(f\"\\nFirst images:\")\n",
    "for i, img_path in enumerate(image_files[:5]):\n",
    "    print(f\"  {i+1}. {img_path.name}\")\n",
    "    \n",
    "if len(image_files) > 5:\n",
    "    print(f\"  ... and {len(image_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b75b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and get basic stats\n",
    "images = []\n",
    "image_stats = []\n",
    "\n",
    "print(\"Loading images...\")\n",
    "for img_path in image_files:\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        images.append(img)\n",
    "        image_stats.append({\n",
    "            'path': img_path,\n",
    "            'name': img_path.name,\n",
    "            'width': img.width,\n",
    "            'height': img.height,\n",
    "            'mean_brightness': img_array.mean(),\n",
    "            'size_bytes': img_path.stat().st_size\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸  Failed to load {img_path.name}: {e}\")\n",
    "\n",
    "print(f\"âœ“ Loaded {len(images)} images successfully\")\n",
    "\n",
    "# Display resolution statistics\n",
    "widths = [s['width'] for s in image_stats]\n",
    "heights = [s['height'] for s in image_stats]\n",
    "\n",
    "print(f\"\\nResolution statistics:\")\n",
    "print(f\"  Width:  {np.min(widths)} - {np.max(widths)} px (mean: {np.mean(widths):.0f})\")\n",
    "print(f\"  Height: {np.min(heights)} - {np.max(heights)} px (mean: {np.mean(heights):.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e1c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "n_display = min(9, len(images))\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_display):\n",
    "    axes[i].imshow(images[i])\n",
    "    axes[i].set_title(f\"{image_stats[i]['name'][:20]}\\n{image_stats[i]['width']}x{image_stats[i]['height']}px\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "for i in range(n_display, 9):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"images\" / \"input_grid.png\", dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Saved input grid to: {OUTPUT_DIR / 'images' / 'input_grid.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe9b84",
   "metadata": {},
   "source": [
    "## 4. Preprocess & Quality Filtering\n",
    "\n",
    "Run quality checks on images:\n",
    "- Size filtering (remove too-small images)\n",
    "- Blur detection (Laplacian variance)\n",
    "- Duplicate detection (perceptual hashing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854013f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: compute blur score\n",
    "def compute_blur_score(image):\n",
    "    \"\"\"Compute Laplacian variance as blur metric. Higher = sharper.\"\"\"\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return laplacian_var\n",
    "\n",
    "# Helper function: compute perceptual hash\n",
    "def compute_phash(image, hash_size=8):\n",
    "    \"\"\"Compute perceptual hash for duplicate detection.\"\"\"\n",
    "    img_gray = image.convert('L').resize((hash_size, hash_size), Image.LANCZOS)\n",
    "    pixels = np.array(img_gray).flatten()\n",
    "    avg = pixels.mean()\n",
    "    diff = pixels > avg\n",
    "    return diff\n",
    "\n",
    "# Helper function: hamming distance between hashes\n",
    "def hamming_distance(hash1, hash2):\n",
    "    \"\"\"Compute hamming distance between two binary arrays.\"\"\"\n",
    "    return np.sum(hash1 != hash2)\n",
    "\n",
    "print(\"Computing quality metrics...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325cfdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add quality metrics to image stats\n",
    "for i, (img, stats) in enumerate(zip(images, image_stats)):\n",
    "    stats['blur_score'] = compute_blur_score(img)\n",
    "    stats['phash'] = compute_phash(img)\n",
    "    stats['quality_score'] = stats['blur_score']  # Can combine multiple factors\n",
    "\n",
    "print(f\"âœ“ Computed quality metrics for {len(image_stats)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89106a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by size\n",
    "size_filtered = []\n",
    "for stats in image_stats:\n",
    "    min_dim = min(stats['width'], stats['height'])\n",
    "    if min_dim >= MIN_IMAGE_SIZE:\n",
    "        size_filtered.append(stats)\n",
    "    else:\n",
    "        print(f\"  Filtered (too small): {stats['name']} ({stats['width']}x{stats['height']})\")\n",
    "\n",
    "print(f\"\\nâœ“ Size filter: {len(size_filtered)}/{len(image_stats)} images passed (min {MIN_IMAGE_SIZE}px)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by blur\n",
    "blur_filtered = []\n",
    "for stats in size_filtered:\n",
    "    if stats['blur_score'] >= BLUR_THRESHOLD:\n",
    "        blur_filtered.append(stats)\n",
    "    else:\n",
    "        print(f\"  Filtered (blurry): {stats['name']} (score: {stats['blur_score']:.1f})\")\n",
    "\n",
    "print(f\"\\nâœ“ Blur filter: {len(blur_filtered)}/{len(size_filtered)} images passed (threshold: {BLUR_THRESHOLD})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter duplicates\n",
    "unique_images = []\n",
    "seen_hashes = []\n",
    "\n",
    "for stats in blur_filtered:\n",
    "    is_duplicate = False\n",
    "    for seen_hash in seen_hashes:\n",
    "        if hamming_distance(stats['phash'], seen_hash) < PERCEPTUAL_HASH_THRESHOLD:\n",
    "            is_duplicate = True\n",
    "            print(f\"  Filtered (duplicate): {stats['name']}\")\n",
    "            break\n",
    "    \n",
    "    if not is_duplicate:\n",
    "        unique_images.append(stats)\n",
    "        seen_hashes.append(stats['phash'])\n",
    "\n",
    "print(f\"\\nâœ“ Duplicate filter: {len(unique_images)}/{len(blur_filtered)} unique images\")\n",
    "print(f\"\\nðŸ“Š Final dataset: {len(unique_images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98279a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best image (hero image) for 3D reconstruction\n",
    "if len(unique_images) == 0:\n",
    "    raise ValueError(\"No images passed quality filters!\")\n",
    "\n",
    "# Sort by quality score (blur score in this case)\n",
    "unique_images_sorted = sorted(unique_images, key=lambda x: x['quality_score'], reverse=True)\n",
    "hero_image_stats = unique_images_sorted[0]\n",
    "hero_image_path = hero_image_stats['path']\n",
    "hero_image = Image.open(hero_image_path).convert('RGB')\n",
    "\n",
    "print(f\"\\nðŸŒŸ Best image selected: {hero_image_stats['name']}\")\n",
    "print(f\"   Resolution: {hero_image_stats['width']}x{hero_image_stats['height']}\")\n",
    "print(f\"   Quality score: {hero_image_stats['quality_score']:.1f}\")\n",
    "print(f\"   Brightness: {hero_image_stats['mean_brightness']:.1f}\")\n",
    "\n",
    "# Display hero image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(hero_image)\n",
    "plt.title(f\"Hero Image: {hero_image_stats['name']}\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"images\" / \"hero_image.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9b963",
   "metadata": {},
   "source": [
    "## 5. Hunyuan3D-2 Inference\n",
    "\n",
    "Run image-to-3D generation using Hunyuan3D-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVICE == \"cpu\":\n",
    "    print(\"âš ï¸  WARNING: Running on CPU. Skipping Hunyuan3D-2 inference.\")\n",
    "    print(\"   This step requires a CUDA-enabled GPU.\")\n",
    "    print(\"   Set DEVICE='cuda' and ensure PyTorch with CUDA is installed.\")\n",
    "    SKIP_INFERENCE = True\n",
    "else:\n",
    "    SKIP_INFERENCE = False\n",
    "    print(f\"âœ“ GPU detected. Proceeding with inference on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INFERENCE:\n",
    "    print(\"Installing/importing Hunyuan3D-2 dependencies...\")\n",
    "    print(\"Note: This may take a few minutes on first run.\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Import Hugging Face libraries\n",
    "        from transformers import AutoModel, AutoProcessor\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        import trimesh\n",
    "        \n",
    "        print(\"âœ“ Dependencies imported successfully\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ Missing dependencies: {e}\")\n",
    "        print(\"\\nPlease install required packages:\")\n",
    "        print(\"  pip install transformers huggingface_hub trimesh\")\n",
    "        SKIP_INFERENCE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e0585",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INFERENCE:\n",
    "    print(f\"Loading Hunyuan3D-2 model: {HUNYUAN_MODEL_ID}\")\n",
    "    print(\"This may download several GB on first run...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Note: The actual API may differ. This is a template.\n",
    "        # Check official Hunyuan3D-2 documentation for exact usage.\n",
    "        \n",
    "        # Placeholder for model loading\n",
    "        print(\"âš ï¸  Model loading code is a template.\")\n",
    "        print(\"   Please refer to Hunyuan3D-2 documentation for exact API.\")\n",
    "        print(\"   Expected: AutoModel.from_pretrained() or similar\\n\")\n",
    "        \n",
    "        # Example structure (may need adjustment):\n",
    "        # model = AutoModel.from_pretrained(HUNYUAN_MODEL_ID)\n",
    "        # processor = AutoProcessor.from_pretrained(HUNYUAN_MODEL_ID)\n",
    "        # model.to(DEVICE)\n",
    "        # model.eval()\n",
    "        \n",
    "        MODEL_LOADED = False  # Set to True when model loads successfully\n",
    "        \n",
    "        print(\"âœ“ Model loading structure ready (needs implementation)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load model: {e}\")\n",
    "        SKIP_INFERENCE = True\n",
    "        MODEL_LOADED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INFERENCE and MODEL_LOADED:\n",
    "    print(\"Running shape generation inference...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Preprocess hero image\n",
    "        # Resize to target size while maintaining aspect ratio\n",
    "        hero_image_resized = hero_image.copy()\n",
    "        hero_image_resized.thumbnail((IMAGE_SIZE, IMAGE_SIZE), Image.LANCZOS)\n",
    "        \n",
    "        print(f\"Processing image: {hero_image_resized.size}\")\n",
    "        \n",
    "        # Run inference (API template - adjust based on actual Hunyuan3D-2 API)\n",
    "        print(\"âš ï¸  Inference code is a template. Adjust for actual API.\\n\")\n",
    "        \n",
    "        # Example structure:\n",
    "        # inputs = processor(images=hero_image_resized, return_tensors=\"pt\").to(DEVICE)\n",
    "        # with torch.no_grad():\n",
    "        #     outputs = model.generate(**inputs, num_inference_steps=50)\n",
    "        # mesh = outputs.mesh  # or similar\n",
    "        \n",
    "        print(\"Shape generation: [TEMPLATE - needs implementation]\")\n",
    "        \n",
    "        SHAPE_GENERATED = False  # Set to True when shape is generated\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Inference failed: {e}\")\n",
    "        SHAPE_GENERATED = False\n",
    "else:\n",
    "    print(\"Skipping inference (no GPU or model not loaded)\")\n",
    "    SHAPE_GENERATED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d188e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INFERENCE and MODEL_LOADED and SHAPE_GENERATED and ENABLE_TEXTURE:\n",
    "    print(\"Running texture generation (optional)...\\n\")\n",
    "    \n",
    "    # Check if enough VRAM available\n",
    "    if torch.cuda.is_available():\n",
    "        free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)\n",
    "        free_gb = free_memory / 1e9\n",
    "        \n",
    "        print(f\"Available GPU memory: {free_gb:.2f} GB\")\n",
    "        \n",
    "        if free_gb < 4.0:\n",
    "            print(\"âš ï¸  Low VRAM. Skipping texture generation.\")\n",
    "            TEXTURE_GENERATED = False\n",
    "        else:\n",
    "            try:\n",
    "                # Run texture generation (API template)\n",
    "                print(\"Texture generation: [TEMPLATE - needs implementation]\\n\")\n",
    "                \n",
    "                # Example:\n",
    "                # textured_mesh = model.add_texture(mesh, hero_image_resized)\n",
    "                \n",
    "                TEXTURE_GENERATED = False  # Set to True when texture is added\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Texture generation failed: {e}\")\n",
    "                TEXTURE_GENERATED = False\n",
    "    else:\n",
    "        TEXTURE_GENERATED = False\n",
    "else:\n",
    "    print(\"Skipping texture generation\")\n",
    "    TEXTURE_GENERATED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702ad9c7",
   "metadata": {},
   "source": [
    "## 6. Export & Visualization\n",
    "\n",
    "Save generated 3D mesh to standard formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597b095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAPE_GENERATED:\n",
    "    print(\"Exporting mesh...\\n\")\n",
    "    \n",
    "    # Export to OBJ format\n",
    "    obj_path = OUTPUT_DIR / \"meshes\" / f\"{CASE_NAME}_mesh.obj\"\n",
    "    \n",
    "    try:\n",
    "        # Export mesh (API template)\n",
    "        print(f\"âš ï¸  Export code is a template.\\n\")\n",
    "        \n",
    "        # Example:\n",
    "        # mesh.export(obj_path)\n",
    "        # or\n",
    "        # trimesh_obj = trimesh.Trimesh(vertices=mesh.vertices, faces=mesh.faces)\n",
    "        # trimesh_obj.export(obj_path)\n",
    "        \n",
    "        print(f\"Would save mesh to: {obj_path}\")\n",
    "        \n",
    "        # Try GLB/GLTF export if supported\n",
    "        glb_path = OUTPUT_DIR / \"meshes\" / f\"{CASE_NAME}_mesh.glb\"\n",
    "        print(f\"Would save GLB to: {glb_path}\")\n",
    "        \n",
    "        MESH_EXPORTED = False  # Set to True when export succeeds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Export failed: {e}\")\n",
    "        MESH_EXPORTED = False\n",
    "else:\n",
    "    print(\"No mesh to export (inference was skipped)\")\n",
    "    MESH_EXPORTED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple metadata file\n",
    "metadata = {\n",
    "    'case_name': CASE_NAME,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'input_images': len(image_files),\n",
    "    'filtered_images': len(unique_images),\n",
    "    'hero_image': hero_image_stats['name'],\n",
    "    'device': DEVICE,\n",
    "    'model': HUNYUAN_MODEL_ID,\n",
    "    'image_size': IMAGE_SIZE,\n",
    "    'shape_generated': SHAPE_GENERATED,\n",
    "    'texture_generated': TEXTURE_GENERATED,\n",
    "    'mesh_exported': MESH_EXPORTED\n",
    "}\n",
    "\n",
    "metadata_path = OUTPUT_DIR / \"metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Saved metadata to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14dc456",
   "metadata": {},
   "source": [
    "## 7. Turntable Render\n",
    "\n",
    "Generate orbit frames and create turntable video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d642dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MESH_EXPORTED:\n",
    "    print(\"Generating turntable renders...\\n\")\n",
    "    \n",
    "    try:\n",
    "        import pyrender\n",
    "        from pyrender import RenderFlags\n",
    "        \n",
    "        print(\"âœ“ PyRender available\")\n",
    "        PYRENDER_AVAILABLE = True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  PyRender not available. Install with: pip install pyrender\")\n",
    "        print(\"   Falling back to basic rendering...\\n\")\n",
    "        PYRENDER_AVAILABLE = False\n",
    "else:\n",
    "    print(\"No mesh available for turntable rendering\")\n",
    "    PYRENDER_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b38b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MESH_EXPORTED and PYRENDER_AVAILABLE:\n",
    "    print(f\"Rendering {TURNTABLE_FRAMES} orbit frames...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load mesh\n",
    "        # mesh_obj = trimesh.load(obj_path)\n",
    "        \n",
    "        # Setup PyRender scene (template)\n",
    "        print(\"âš ï¸  Rendering code is a template.\\n\")\n",
    "        \n",
    "        # Example structure:\n",
    "        # scene = pyrender.Scene()\n",
    "        # mesh_node = pyrender.Mesh.from_trimesh(mesh_obj)\n",
    "        # scene.add(mesh_node)\n",
    "        \n",
    "        # camera_pose = np.eye(4)\n",
    "        # camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0)\n",
    "        # scene.add(camera, pose=camera_pose)\n",
    "        \n",
    "        # light = pyrender.DirectionalLight(color=np.ones(3), intensity=3.0)\n",
    "        # scene.add(light, pose=camera_pose)\n",
    "        \n",
    "        # renderer = pyrender.OffscreenRenderer(512, 512)\n",
    "        \n",
    "        render_frames = []\n",
    "        \n",
    "        for i in range(TURNTABLE_FRAMES):\n",
    "            angle = 2 * np.pi * i / TURNTABLE_FRAMES\n",
    "            \n",
    "            # Rotate camera around object\n",
    "            # Update camera pose based on angle\n",
    "            # color, depth = renderer.render(scene)\n",
    "            # render_frames.append(color)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Rendered frame {i+1}/{TURNTABLE_FRAMES}\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Rendered {TURNTABLE_FRAMES} frames\")\n",
    "        FRAMES_RENDERED = False  # Set to True when rendering succeeds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Rendering failed: {e}\")\n",
    "        FRAMES_RENDERED = False\n",
    "else:\n",
    "    print(\"Skipping turntable rendering\")\n",
    "    FRAMES_RENDERED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2320e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FRAMES_RENDERED:\n",
    "    print(\"Creating turntable video...\\n\")\n",
    "    \n",
    "    try:\n",
    "        import imageio\n",
    "        \n",
    "        video_path = OUTPUT_DIR / \"turntable.mp4\"\n",
    "        \n",
    "        # Write video\n",
    "        # writer = imageio.get_writer(video_path, fps=TURNTABLE_FPS)\n",
    "        # for frame in render_frames:\n",
    "        #     writer.append_data(frame)\n",
    "        # writer.close()\n",
    "        \n",
    "        print(f\"âœ“ Turntable video would be saved to: {video_path}\")\n",
    "        print(f\"   {TURNTABLE_FRAMES} frames @ {TURNTABLE_FPS} FPS\")\n",
    "        \n",
    "        # Display video in notebook\n",
    "        # display(Video(video_path, embed=True, width=600))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Video creation failed: {e}\")\n",
    "        print(\"   Install imageio: pip install imageio[ffmpeg]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4628e",
   "metadata": {},
   "source": [
    "## 8. Nerfstudio Notes (Optional)\n",
    "\n",
    "For NeRF-based reconstruction, you can use **Nerfstudio** with your filtered image set.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "pip install nerfstudio\n",
    "```\n",
    "\n",
    "### Basic workflow\n",
    "\n",
    "1. **Process images with COLMAP** (structure-from-motion):\n",
    "   ```bash\n",
    "   ns-process-data images --data data/raw/dog_test_01 --output-dir data/processed/dog_test_01\n",
    "   ```\n",
    "\n",
    "2. **Train NeRF model** (e.g., nerfacto):\n",
    "   ```bash\n",
    "   ns-train nerfacto --data data/processed/dog_test_01\n",
    "   ```\n",
    "\n",
    "3. **Export mesh**:\n",
    "   ```bash\n",
    "   ns-export poisson --load-config outputs/dog_test_01/nerfacto/config.yml --output-dir outputs/dog_test_01/mesh\n",
    "   ```\n",
    "\n",
    "4. **View in browser**:\n",
    "   - Training automatically launches viewer at `http://localhost:7007`\n",
    "\n",
    "### Requirements for NeRF\n",
    "\n",
    "- Multiple views (10+ images) from different angles\n",
    "- Good camera coverage around the object\n",
    "- Consistent lighting\n",
    "- Static scene (no moving objects)\n",
    "\n",
    "### Comparison: Single-image 3D vs. NeRF\n",
    "\n",
    "| Method | Images | Quality | Speed | Use Case |\n",
    "|--------|--------|---------|-------|----------|\n",
    "| Hunyuan3D-2 | 1 | Good | Fast | Quick previews, single-view data |\n",
    "| NeRF | 10-100+ | Excellent | Slow | High-quality, multi-view data |\n",
    "\n",
    "For this smoketest, we focus on **Hunyuan3D-2** for single-image reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d9327",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Generate final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b118bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\" SMOKETEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCase: {CASE_NAME}\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nEnvironment:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Input images: {len(image_files)}\")\n",
    "print(f\"  After filtering: {len(unique_images)}\")\n",
    "print(f\"  Hero image: {hero_image_stats['name']}\")\n",
    "print(f\"\\nPipeline Results:\")\n",
    "print(f\"  âœ“ Quality checks passed\")\n",
    "print(f\"  {'âœ“' if SHAPE_GENERATED else 'âœ—'} Shape generation\")\n",
    "print(f\"  {'âœ“' if TEXTURE_GENERATED else 'âœ—'} Texture generation\")\n",
    "print(f\"  {'âœ“' if MESH_EXPORTED else 'âœ—'} Mesh export\")\n",
    "print(f\"  {'âœ“' if FRAMES_RENDERED else 'âœ—'} Turntable rendering\")\n",
    "print(f\"\\nOutputs saved to: {OUTPUT_DIR}\")\n",
    "print(f\"  - metadata.json\")\n",
    "print(f\"  - images/\")\n",
    "print(f\"  - meshes/\")\n",
    "print(f\"  - renders/\")\n",
    "if FRAMES_RENDERED:\n",
    "    print(f\"  - turntable.mp4\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "if not SHAPE_GENERATED:\n",
    "    print(\"\\nâš ï¸  NOTE: Full pipeline requires GPU with CUDA\")\n",
    "    print(\"   This run completed quality checks only.\")\n",
    "    print(\"   Re-run on GPU-enabled machine for 3D generation.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
